#!/usr/bin/env python3
"""
Investigate violations output and check for errata by reading original parquet files.

This script:
1. Reads the violations CSV output
2. Selects sample rows (especially those with violations)
3. Reads the corresponding rows from the original parquet files using SHA256
4. Displays the original text and checks for parsing errors/errata
"""

import argparse
import ast
import csv
import logging
import sys
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd

# Constants
CONTEXT_WINDOW_SIZE = 200  # Characters of context around violations
OCR_ERROR_PATTERNS = ['lllllll', '|||||||']  # Common OCR error patterns

# Set up logger
logger = logging.getLogger(__name__)


def load_violations_csv(csv_path: str) -> pd.DataFrame:
    """Load violations CSV file."""
    logger.info(f"Loading violations CSV from: {csv_path}")
    df = pd.read_csv(csv_path)
    logger.info(f"Loaded {len(df)} records")
    logger.info(f"Records with violations: {len(df[df['num_violations'] > 0])}")
    return df


def find_document_in_parquet(sha256: str, parquet_dir: str) -> Optional[Dict]:
    """Find a document in parquet files by SHA256 hash."""
    parquet_path = Path(parquet_dir)
    parquet_files = list(parquet_path.glob("*.parquet"))
    
    for parquet_file in parquet_files:
        try:
            df = pd.read_parquet(parquet_file)
            matches = df[df['sha256'] == sha256]
            
            if not matches.empty:
                row = matches.iloc[0]
                return {
                    'sha256': row['sha256'],
                    'dateprocessed': row['dateprocessed'],
                    'text': row['text'],
                    'parquet_file': parquet_file.name
                }
        except Exception as e:
            logger.error(f"Error reading {parquet_file.name}: {e}")
            continue
    
    return None


def display_document_details(violation_record: Dict, original_doc: Dict) -> None:
    """Display details of a document for manual inspection."""
    print("\n" + "="*80)
    print("DOCUMENT INSPECTION")
    print("="*80)
    
    print("\n--- VIOLATION RECORD ---")
    print(f"SHA256: {violation_record['sha256']}")
    print(f"Agency ID: {violation_record['agency_id']}")
    print(f"Agency Name: {violation_record['agency_name']}")
    print(f"Date: {violation_record['date']}")
    print(f"Number of Violations: {violation_record['num_violations']}")
    print(f"Violations List: {violation_record['violations_list']}")
    
    if original_doc:
        print("\n--- ORIGINAL PARQUET DATA ---")
        print(f"Parquet File: {original_doc['parquet_file']}")
        print(f"Date Processed: {original_doc['dateprocessed']}")
        
        # Parse text
        try:
            # Handle both string and array formats
            text_data = original_doc['text']
            if isinstance(text_data, str):
                # Parse string representation safely
                # Note: parquet files generated by this pipeline are trusted
                # In production with untrusted sources, use json.loads instead
                text_pages = ast.literal_eval(text_data)
            else:
                # It's already an array (numpy array or list)
                text_pages = list(text_data)
            
            print(f"Number of Pages: {len(text_pages)}")
            
            # Display first few pages
            print("\n--- FIRST PAGE (first 2000 chars) ---")
            if text_pages:
                first_page = text_pages[0][:2000]
                print(first_page)
                print("...")
            
            # Search for key sections
            full_text = '\n'.join(text_pages)
            
            # Check for license number
            print("\n--- CHECKING FOR KEY FIELDS ---")
            if violation_record['agency_id']:
                if violation_record['agency_id'] in full_text:
                    print(f"✓ Agency ID '{violation_record['agency_id']}' found in text")
                else:
                    print(f"✗ Agency ID '{violation_record['agency_id']}' NOT found in text [POTENTIAL ERRATA]")
            
            # Check for agency name
            if violation_record['agency_name']:
                if violation_record['agency_name'] in full_text:
                    print(f"✓ Agency Name '{violation_record['agency_name']}' found in text")
                else:
                    # Check for partial match
                    name_parts = violation_record['agency_name'].split()
                    found_parts = sum(1 for part in name_parts if len(part) > 3 and part in full_text)
                    if found_parts > len(name_parts) / 2:
                        print(f"~ Agency Name partially found ({found_parts}/{len(name_parts)} parts)")
                    else:
                        print(f"✗ Agency Name '{violation_record['agency_name']}' NOT found in text [POTENTIAL ERRATA]")
            
            # Check for violations
            if violation_record['violations_list'] and not pd.isna(violation_record['violations_list']):
                violations = violation_record['violations_list'].split('; ')
                print(f"\n--- CHECKING VIOLATIONS ({len(violations)} total) ---")
                for i, violation in enumerate(violations[:5], 1):  # Check first 5
                    if violation in full_text:
                        print(f"  {i}. ✓ '{violation}' found in text")
                    else:
                        # Try to find similar patterns (but be aware of false positives)
                        # Note: This is a heuristic check and may match unintended text
                        violation_clean = violation.replace('R ', '').replace('MCL ', '')
                        if violation_clean in full_text:
                            print(f"  {i}. ~ '{violation}' found (without prefix - may be false positive)")
                        else:
                            print(f"  {i}. ✗ '{violation}' NOT clearly found [POTENTIAL ERRATA]")
            
            # Look for common errata patterns
            print("\n--- CHECKING FOR COMMON ERRATA PATTERNS ---")
            
            # Check for "not violated" near violations
            if violation_record['violations_list'] and not pd.isna(violation_record['violations_list']):
                violations = violation_record['violations_list'].split('; ')
                for violation in violations[:3]:
                    if violation in full_text:
                        # Find position and check context
                        pos = full_text.find(violation)
                        context = full_text[max(0, pos-CONTEXT_WINDOW_SIZE):min(len(full_text), pos+CONTEXT_WINDOW_SIZE)]
                        if 'not violated' in context.lower() or 'not in violation' in context.lower():
                            print(f"⚠ WARNING: '{violation}' appears near 'not violated' text [LIKELY ERRATA]")
                            print(f"   Context: ...{context[:100]}...")
            
            # Check for OCR errors (common patterns)
            for pattern in OCR_ERROR_PATTERNS:
                if pattern in full_text:
                    print(f"⚠ Possible OCR error: Pattern '{pattern}' detected")
                    break
            
        except Exception as e:
            logger.error(f"Error parsing text: {e}")
            print(f"\n✗ ERROR parsing text: {e}")
    else:
        print("\n✗ Original document NOT FOUND in parquet files [ERROR]")
    
    print("\n" + "="*80)


def investigate_violations(violations_csv: str, parquet_dir: str, sample_size: int = 10, interactive: bool = True) -> None:
    """Investigate violations output against original parquet data."""
    # Load violations CSV
    df = load_violations_csv(violations_csv)
    
    # Select samples
    # 1. Some with violations
    with_violations = df[df['num_violations'] > 0].head(sample_size // 2)
    # 2. Some without violations (as control)
    without_violations = df[df['num_violations'] == 0].head(sample_size // 2)
    
    samples = pd.concat([with_violations, without_violations])
    
    logger.info(f"\nInvestigating {len(samples)} sample documents:")
    logger.info(f"  - {len(with_violations)} with violations")
    logger.info(f"  - {len(without_violations)} without violations")
    
    # Statistics
    missing_docs = 0
    
    for idx, row in samples.iterrows():
        violation_record = {
            'sha256': row['sha256'],
            'agency_id': row['agency_id'],
            'agency_name': row['agency_name'],
            'date': row['date'],
            'num_violations': row['num_violations'],
            'violations_list': row['violations_list']
        }
        
        # Find original document
        original_doc = find_document_in_parquet(row['sha256'], parquet_dir)
        
        if original_doc is None:
            missing_docs += 1
        
        # Display details
        display_document_details(violation_record, original_doc)
        
        # Wait for user to review (if interactive)
        if interactive:
            input("\nPress Enter to continue to next document (or Ctrl+C to stop)...")
    
    # Summary
    print("\n" + "="*80)
    print("INVESTIGATION SUMMARY")
    print("="*80)
    print(f"Documents investigated: {len(samples)}")
    print(f"Documents with violations: {len(with_violations)}")
    print(f"Documents without violations: {len(without_violations)}")
    print(f"Documents not found in parquet: {missing_docs}")
    print("\nReview the output above to identify any errata (parsing errors).")
    print("Look for:")
    print("  - Violations marked that appear near 'not violated' text")
    print("  - Agency IDs or names that don't match the text")
    print("  - Violations that can't be found in the original text")
    print("="*80)


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Investigate violations output for errata by comparing with original parquet data"
    )
    parser.add_argument(
        "--violations-csv",
        default="violations_output.csv",
        help="Path to violations CSV file (default: violations_output.csv)"
    )
    parser.add_argument(
        "--parquet-dir",
        default="pdf_parsing/parquet_files",
        help="Directory containing parquet files (default: pdf_parsing/parquet_files)"
    )
    parser.add_argument(
        "--sample-size",
        type=int,
        default=10,
        help="Number of documents to investigate (default: 10)"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose debug output"
    )
    parser.add_argument(
        "--non-interactive",
        action="store_true",
        help="Run without pausing between documents"
    )
    
    args = parser.parse_args()
    
    # Configure logging
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # Check if files exist
    if not Path(args.violations_csv).exists():
        logger.error(f"Violations CSV not found: {args.violations_csv}")
        logger.info("Run: python3 parse_parquet_violations.py --parquet-dir pdf_parsing/parquet_files -o violations_output.csv")
        sys.exit(1)
    
    if not Path(args.parquet_dir).exists():
        logger.error(f"Parquet directory not found: {args.parquet_dir}")
        sys.exit(1)
    
    investigate_violations(args.violations_csv, args.parquet_dir, args.sample_size, interactive=not args.non_interactive)


if __name__ == "__main__":
    main()
